# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04 copy minibatch_training.ipynb.

# %% auto 0
__all__ = ['accuracy', 'report', 'Dataset', 'DataLoader', 'fit', 'get_dls']

# %% ../nbs/04 copy minibatch_training.ipynb 1
import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt
from pathlib import Path
from torch import tensor,nn
import torch.nn.functional as F

# %% ../nbs/04 copy minibatch_training.ipynb 23
def accuracy(preds, yb):
    return (preds.argmax(dim=1) == yb).float().mean()

# %% ../nbs/04 copy minibatch_training.ipynb 24
def report(loss, acts, yb): print(f'{loss:.2f}, {accuracy(acts, yb):.2f}')

# %% ../nbs/04 copy minibatch_training.ipynb 67
class Dataset():
    def __init__(self, x, y):
        assert (len(x) == len(y))
        self.x=x
        self.y=y
    
    def __getitem__(self, idx):
        return (self.x[idx], self.y[idx])
    
    def __len__(self):
        return len(self.x)
        

# %% ../nbs/04 copy minibatch_training.ipynb 75
class DataLoader():
    def __init__(self, dataset, bs):
        self.dataset = dataset
        self.bs = bs
    
    def __iter__(self):
        for i in range(0, len(self.dataset), self.bs):
            yield self.dataset[i : i+bs]

# %% ../nbs/04 copy minibatch_training.ipynb 111
from torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler

# %% ../nbs/04 copy minibatch_training.ipynb 122
'''
model.train and model.eval set configs for things like normalization and dropout to act appropriately
for each phase
'''
def fit(epochs, model, loss_func, opt, dl_train, dl_valid):
    for epoch in range(epochs):
        model.train()
        for xb, yb in dl_train:
            loss=loss_func(model(xb), yb)
            loss.backward()
            opt.step()
            opt.zero_grad()
        
        model.eval()
        with torch.no_grad():
            totloss,totacc,count=0.,0.,0.
            for xb,yb in dl_valid:
                acts= model(xb)
                loss= loss_func(acts,yb)
                n=xb.shape[0]
                count += n
                totloss += loss.item()*n # doing this gets a weighted average of loss across batches: last batch may be smaller
                totacc += accuracy(acts, yb).item() * n
        print(f'epoch: {epoch}, val loss: {totloss/count:2f}, val accuracy: {totacc/count:2f}')

# %% ../nbs/04 copy minibatch_training.ipynb 123
def get_dls(ds_train, ds_valid, bs, **kwargs):
    return (DataLoader(ds_train, batch_size=bs, shuffle=True, **kwargs),
            DataLoader(ds_valid, batch_size=bs, shuffle=False, **kwargs))
