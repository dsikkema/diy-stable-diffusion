# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/28 copy attention only.ipynb.

# %% auto 0
__all__ = ['abar', 'inv_abar', 'noisify', 'collate_noised', 'dl', 'timestep_embedding', 'pre_conv', 'upsample_and_conv', 'lin',
           'SelfAttention2D', 'EmbResBlock', 'saved', 'DownBlock', 'UpBlock', 'EmbUNetModel', 'ddim_step', 'sample',
           'diffusion_class_collate', 'CondEmbUNetModel', 'cond_sample']

# %% ../nbs/28 copy attention only.ipynb 1
import math, torch
from datasets.load import load_dataset
import fastcore.all as fc
from pathlib import Path
from functools import partial
from fastprogress import progress_bar
import matplotlib.pyplot as plt
from torch.utils.data import default_collate,DataLoader
import torch.nn.functional as F
import torchvision.transforms.functional as TF
import matplotlib as mpl
from .datasets import *
from .conv import *
from .learner import *
from .resnet import *

from torch.optim import lr_scheduler

import torch.nn as nn

import torch.optim

# %% ../nbs/28 copy attention only.ipynb 5
'''
continous noise on cosine schedule
'''
def abar(t):
    return (t*math.pi / 2).cos()**2

def inv_abar(x):
    return x.sqrt().acos() * 2/math.pi

def noisify(x0):
    dev = x0.device
    t = torch.rand((x0.shape[0],), device=dev).clamp(0,0.999)
    noise=torch.randn_like(x0)
    abar_t=abar(t)[:,None, None, None]
    x_t = abar_t.sqrt() * x0 + (1-abar_t).sqrt() * noise
    return (x_t, t,), noise

def collate_noised(batch):
    return noisify(default_collate(batch)['image'])

def dl(dataset, bs, collate_fn=collate_noised):
    return DataLoader(dataset, batch_size=bs, collate_fn=collate_fn, num_workers=4)


# %% ../nbs/28 copy attention only.ipynb 8
def timestep_embedding(timesteps, emb_dim, max_period=10_000):
    exponent = -math.log(max_period) * torch.linspace(0, 1, emb_dim //2, device = timesteps.device)
    emb = timesteps[:, None].float() * exponent.exp()[None, :]
    emb = torch.cat([emb.sin(), emb.cos()], dim=-1)
    if emb_dim % 2 == 1:
        emb = F.pad(emb, (0, 1, 0, 0))
    
    return emb

# %% ../nbs/28 copy attention only.ipynb 9
'''
conv with pre-activation
'''
def pre_conv(in_channels, out_channels, ks=3, stride=1, act=nn.SiLU, norm=None, bias=True):
    layers = nn.Sequential()
    if norm:
        layers.append(norm(in_channels))
    
    if act:
        layers.append(act())
    
    layers.append(nn.Conv2d(in_channels, out_channels, ks, stride, bias=bias, padding=ks//2))
    return layers

# %% ../nbs/28 copy attention only.ipynb 10
'''
Upscale by factor of 2 and do a convolution
'''
def upsample_and_conv(channels):
    return nn.Sequential(
        nn.Upsample(scale_factor=2.0),
        nn.Conv2d(channels, channels, 3, padding=1) # 1 == (ks=3)//2
    )

# %% ../nbs/28 copy attention only.ipynb 11
def lin(n_inp, n_out, act=nn.SiLU, norm=None, bias=True):
    layers= nn.Sequential()
    if norm:
        layers.append(norm(n_inp))
    
    if act:
        layers.append(act())
    
    layers.append(nn.Linear(n_inp, n_out, bias=bias))
    return layers

# %% ../nbs/28 copy attention only.ipynb 14
class SelfAttention2D(nn.Module):
    def __init__(self, channels, channels_per_head):
        super().__init__()
        self.n_heads = channels//channels_per_head
        self.q = lin(channels, channels)
        self.k = lin(channels, channels)
        self.v = lin(channels, channels)
        '''
        If Q_x and K_x matrices (size = (pixels, channels_per_head)) both have 0 mean unit variance, by property of adding variances,
        the output will have variance=channels_per_head and std=sqrt(channels_per_head). Divide result by std to make result have
        std=1. Helps preserve balance of values before taking softmax so some values don't predominate.
        '''
        self.scale = math.sqrt(channels_per_head)
        self.last_lin = lin(channels, channels)
        self.norm = nn.LayerNorm(channels)
    
    def forward(self, x):
        n, c, h, w = x.shape
        x=x.view(n, c, h*w).transpose(1,2)
        x=self.norm(x)
        q_x=self.q(x)
        k_x=self.k(x)
        v_x=self.v(x)
        x=self._to_heads(x)
        q_x,k_x,v_x = map(self._to_heads, (q_x, k_x, v_x))
        s = (q_x @ k_x.transpose(1, 2)) / self.scale
        s = s.softmax(dim=-1)
        result = s @ v_x
        result = self._to_batch(result)
        
        # we're doing skip connection outside of attention block, not inside it.
        return self.last_lin(result).view(n, c, h, w)
    
    def _to_heads(self, tsr):
        n, s, c = tsr.shape
        return tsr.transpose(1,2).reshape(n*self.n_heads, c//self.n_heads, s).transpose(1,2)
    
    def _to_batch(self, tsr):
        n, s, c = tsr.shape
        return tsr.transpose(1, 2).reshape(n//self.n_heads, c*self.n_heads, s).transpose(1,2)

# %% ../nbs/28 copy attention only.ipynb 19
class EmbResBlock(nn.Module):
    def __init__(self, n_emb, in_channels, out_channels, ks=3, act=nn.SiLU, norm=nn.BatchNorm2d, channels_per_attn_head=0):
        super().__init__()
        self.in_channels=in_channels
        self.out_channels=out_channels
        
        # half the result of this will be used to scale, the other half to shift
        self.emb_lin = nn.Linear(n_emb, out_channels * 2)
        self.conv1 = pre_conv(in_channels, out_channels, ks, act=act, norm=norm)
        self.conv1.in_channels = in_channels
        self.conv1.out_channels = out_channels
        self.conv2 = pre_conv(out_channels, out_channels, ks, act=act, norm=norm)
        
        # This block not used for up- or down- sampling, but can change channel count, hence the possible idconv
        self.idconv = fc.noop if in_channels==out_channels else nn.Conv2d(out_channels, out_channels, 1)
        
        if channels_per_attn_head > 0:
            self.attn_layer = SelfAttention2D(out_channels, channels_per_attn_head)
        else:
            self.attn_layer = None
    
    def forward(self, x, emb):
        inp = x
        # import pdb; pdb.set_trace()
        x=self.conv1(x)
        t_emb=self.emb_lin(F.silu(emb))
        scale,shift = torch.chunk(t_emb[:, :, None, None], 2, dim=1)
        x= x*(1+scale) + shift
        x=self.conv2(x)
        
        # The first residual
        x=x+self.idconv(x)
        
        if self.attn_layer:
            # a second residual layer dedicated to attention
            x=x + self.attn_layer(x)
        return x
        

# %% ../nbs/28 copy attention only.ipynb 20
def saved(module, block, save_in=False):
    fwd = module.forward
    
    # is this annotation needed?
    # @wraps(module.forward)
    def _f(*args, **kwargs):
        out = fwd(*args, **kwargs)
        
        block.saved_out.append(out)
        
        # saved input only for debugging purposes
        if save_in:
            block.saved_in.append(args[0])
        return out
    
    module.forward = _f
    return module

# %% ../nbs/28 copy attention only.ipynb 21
class DownBlock(nn.Module):
    def __init__(self, n_t_emb, in_channels, out_channels, is_final_down=False, num_layers=1, channels_per_head=0):
        super().__init__()
        
        self.in_channels = in_channels
        self.out_channels = out_channels
        # make EmbResNets
        self.erbs = []
        for i in range(0, num_layers):
            # first erb expands channels, subsequent ones keep it constant
            erb_in_channels = in_channels if i==0 else out_channels
            erb=EmbResBlock(n_t_emb, erb_in_channels, out_channels, channels_per_attn_head=channels_per_head)
            self.erbs.append(saved(erb, self))
        self.erbs = nn.ModuleList(self.erbs)
        
        # only downsample if not the final downblock
        self.is_final_down=is_final_down
        if not self.is_final_down:
            kernel_size=3
            down_conv = nn.Conv2d(out_channels, out_channels, kernel_size, stride=2, padding=kernel_size//2)
            self.down_conv = saved(down_conv, self)
        else:
            self.down_conv = None
            
    def forward(self, x, emb):
        # saved io only needed during scope of a single forward pass through module. The next time
        # forward is called will be the next minibatch at which the data needs to be reset.
        self.saved_in=[]
        self.saved_out=[]
        for erb in self.erbs:
            x = erb(x, emb)
        
        if not self.is_final_down:
            x = self.down_conv(x)
        
        return x

# %% ../nbs/28 copy attention only.ipynb 22
class UpBlock(nn.Module):
    def __init__(self, n_t_emb, prev_block_out_chans, skip_in_channels_ls, out_channels, is_final_up=False, num_layers=1, channels_per_head=0):
        super().__init__()
        self.in_channels = prev_block_out_chans
        self.out_channels = out_channels
        
        num_layers = len(skip_in_channels_ls)
        
        # make EmbResNets
        self.erbs = []
        for i in range(0, num_layers):
            
            '''
            if first erb in block, then data coming upward through the block will have come from the previous block
            
            otherwise will be from the last erb in *this* block
            '''
            if i==0:
                upward_in_channels=prev_block_out_chans
            else:
                upward_in_channels=out_channels
                
            '''
            skip connection is input concatenated on the channel dimension, so each erb must take channels
            equal to those from the data being passed up plus those from the data being concatenated onto it
            '''
            erb_in_channels = upward_in_channels + skip_in_channels_ls[i]
            erb=EmbResBlock(n_t_emb, erb_in_channels, out_channels, channels_per_attn_head=channels_per_head)
            self.erbs.append(saved(erb, self))
        self.erbs = nn.ModuleList(self.erbs)
        
        # only upsample if not the final upblock
        self.is_final_up=is_final_up
        if not self.is_final_up:
            up_conv = upsample_and_conv(out_channels)
            self.up_conv = saved(up_conv, self)
        else:
            self.up_conv = None
            
    def forward(self, x, emb, skip_conns):
        self.saved_in=[]
        self.saved_out=[]
        for erb in self.erbs:
            x = erb(torch.cat([x, skip_conns.pop()], dim=1), emb)
        
        if not self.is_final_up:
            x = self.up_conv(x)
        return x

# %% ../nbs/28 copy attention only.ipynb 24
class EmbUNetModel(nn.Module):
    def __init__(self, in_channels=1, out_channels=1, block_channel_counts=(32,64, 128, 256), num_layers=1, channels_per_head=8, attn_start=1):
        super().__init__()
        # in conv
        self.first_conv = conv(in_channels, block_channel_counts[0], stride=1, act=False)
        self.first_conv.in_channels=in_channels
        self.first_conv.out_channels=block_channel_counts[0]
        
        # timestep embedding
        '''
        length of embedding produced by initial timestep embedding
        only used to produce transformed t embeddings
        '''
        self.t_emb_sz = block_channel_counts[0]
        
        '''
        length of embedding after being transformed by an MLP to be passed in to all ERBs in all 
        down and up blocks
        '''
        final_emb_sz = self.t_emb_sz * 4
        
        self.emb_mlp = nn.Sequential(lin(self.t_emb_sz, final_emb_sz, norm=nn.BatchNorm1d),
                                  lin(final_emb_sz, final_emb_sz))
        
        # down blocks
        num_blocks = len(block_channel_counts)
        final=False
        self.downblocks = []
        down_in_channels = block_channel_counts[0]
        for i in range(num_blocks):
            if i==num_blocks - 1:
                final = True
            down_out_channels = block_channel_counts[i]
            erb_channels_per_head = channels_per_head if i >= attn_start else 0
            block = DownBlock(final_emb_sz, down_in_channels, down_out_channels, final, num_layers, erb_channels_per_head)
            down_in_channels=down_out_channels
                              
            self.downblocks.append(block)
            
        self.downblocks = nn.ModuleList(self.downblocks)
        
        # mid block
        self.midblock = EmbResBlock(final_emb_sz, block_channel_counts[-1], block_channel_counts[-1], channels_per_attn_head=channels_per_head)
        
        # up blocks
        
        '''
        calculate the channels of skip connections:
        
        start with a single entry of first channel block size (output of first_conv)
        
        subsequently for all but last downblock, add (num_layers+1) copies of corresponding channel count for block,
        for the num_layers of resblock and the 1 downsampling layer, all of which produce the same out_channels
        
        finally add num_layers copies of the final block channel_out, because last downblock has no downsampling
        
        length of list will be, therefore, 1 + (num_blocks-1)*(num_layers+1) + num_layers = num_blocks*(num_layers+1),
        with (num_layers+1) being the number of skip connections going into each upblock
        '''
        skip_conn_channels=[block_channel_counts[0]]
        for i in range(num_blocks - 1):
            skip_conn_channels = skip_conn_channels + (num_layers+1) * [block_channel_counts[i]]
        skip_conn_channels = skip_conn_channels + num_layers * [block_channel_counts[-1]]
        
        skips_per_block = num_layers+1
        skip_conn_channels = list(reversed(skip_conn_channels))
        skip_conn_channels = [skip_conn_channels[i: i+skips_per_block] for i in range(0, len(skip_conn_channels), skips_per_block)]
        self.skip_conn_channels = skip_conn_channels
            
        prev_channels = block_channel_counts[-1]
        final = False
        self.upblocks=[]
        for i in range(num_blocks):
            if i==num_blocks-1:
                final = True
            
            erb_channels_per_head = channels_per_head if i < (num_blocks - attn_start) else 0
            up_out_channels=block_channel_counts[-(i+1)]
            block = UpBlock(final_emb_sz, prev_channels, skip_conn_channels[i], up_out_channels, final, num_layers+1, erb_channels_per_head)
            self.upblocks.append(block)
            prev_channels = up_out_channels
        
        self.upblocks = nn.ModuleList(self.upblocks)
        
        # last conv
        self.last_conv= pre_conv(block_channel_counts[0], out_channels, stride=1, act=nn.SiLU, norm=nn.BatchNorm2d, bias=False)
        self.last_conv.in_channels = block_channel_counts[0]
        self.last_conv.out_channels = out_channels
    def forward(self, inp):
        x, t = inp
        t_emb = timestep_embedding(t, self.t_emb_sz)
        generic_emb = self.emb_mlp(t_emb)
        x = self.first_conv(x)
        skip_conns = [x]
        for block in self.downblocks:
            x = block(x, generic_emb)
            skip_conns = skip_conns + block.saved_out
        
        x = self.midblock(x, generic_emb)
        
        for block  in self.upblocks:
            x = block(x, generic_emb, skip_conns)
        
        return self.last_conv(x)
        

# %% ../nbs/28 copy attention only.ipynb 55
def ddim_step(xt, noise, abar_t, abar_tm1, bbar_t, bbar_tm1, eta, sigma, clamp=True):
    sigma = ((bbar_tm1 / bbar_t).sqrt() * (bbar_t / abar_tm1).sqrt()) * eta
    '''
    image as derived from predicted noise
    '''
    x0hat = (xt - noise*bbar_t.sqrt()) / abar_t.sqrt() 
    
    if clamp:
        x0hat = x0hat.clamp(-1, 1)
        
    if bbar_tm1 <= sigma**2 + 0.01:
        sigma = 0.0
    
    xt = abar_tm1.sqrt() * x0hat + (bbar_tm1 - sigma**2).sqrt() * noise
    xt += sigma * torch.randn(xt.shape).to(xt.device)
    
    return x0hat, xt

# %% ../nbs/28 copy attention only.ipynb 56
def sample(denoise_fn, denoise_mdl, sz, steps, eta=1., clamp=True):
    denoise_mdl.eval()
    xt = torch.randn(sz).cuda()
    step_sz = 1/steps
    ts = torch.linspace(1-step_sz, 0, steps).cuda()
    for i,t in enumerate(progress_bar(ts)):
        with torch.no_grad():
            pred_noise = denoise_mdl((xt, t[None]))
            abar_t=abar(t)
            abar_tm1=abar(t - step_sz) if t >= step_sz else torch.tensor(1).cuda()
            x0hat, xt = denoise_fn(xt, pred_noise, abar_t, abar_tm1, 1-abar_t, 1-abar_tm1, eta, 1-((i+1)/100), clamp=clamp)
    return x0hat.float().cpu()
        

# %% ../nbs/28 copy attention only.ipynb 62
def diffusion_class_collate(batch):
    batch = default_collate(batch)
    (xt, t), added_noise = noisify(batch['image'])
    return (xt, t, batch['label']), added_noise


# %% ../nbs/28 copy attention only.ipynb 64
class CondEmbUNetModel(nn.Module):
    def __init__(self, num_classes, in_channels=1, out_channels=1, block_channel_counts=(32,64, 128, 256), num_layers=1, channels_per_head=8, attn_start=1):
        super().__init__()
        # in conv
        self.first_conv = conv(in_channels, block_channel_counts[0], stride=1, act=False)
        self.first_conv.in_channels=in_channels
        self.first_conv.out_channels=block_channel_counts[0]
        
        # timestep embedding
        '''
        length of embedding produced by initial timestep embedding
        only used to produce transformed t embeddings
        
        this size doesn't particularly have to match any channel size because it will
        get re-transformed by the MLP
        '''
        self.t_emb_sz = block_channel_counts[0]
        
        '''
        length of embedding to be passed in to all ERBs in all 
        down and up blocks.
        
        This will be the size of the single information embedding which will be the sum
        of the class embedding and the mlp-transformed time-embedding
        '''
        final_emb_sz = self.t_emb_sz * 4
        
        self.t_emb_mlp = nn.Sequential(lin(self.t_emb_sz, final_emb_sz, norm=nn.BatchNorm1d),
                                  lin(final_emb_sz, final_emb_sz))
        
        # class embedding
        '''
        nn.Embedding used because it does one-hot encoding itself, takes an index in forward pass
        and produced embedded vector
        '''
        self.c_emb = nn.Embedding(num_classes, final_emb_sz)
        
        # down blocks
        num_blocks = len(block_channel_counts)
        final=False
        self.downblocks = []
        down_in_channels = block_channel_counts[0]
        for i in range(num_blocks):
            if i==num_blocks - 1:
                final = True
            down_out_channels = block_channel_counts[i]
            erb_channels_per_head = channels_per_head if i >= attn_start else 0
            block = DownBlock(final_emb_sz, down_in_channels, down_out_channels, final, num_layers, erb_channels_per_head)
            down_in_channels=down_out_channels
                              
            self.downblocks.append(block)
            
        self.downblocks = nn.ModuleList(self.downblocks)
        
        # mid block
        self.midblock = EmbResBlock(final_emb_sz, block_channel_counts[-1], block_channel_counts[-1], channels_per_attn_head=channels_per_head)
        
        # up blocks
        
        '''
        calculate the channels of skip connections:
        
        start with a single entry of first channel block size (output of first_conv)
        
        subsequently for all but last downblock, add (num_layers+1) copies of corresponding channel count for block,
        for the num_layers of resblock and the 1 downsampling layer, all of which produce the same out_channels
        
        finally add num_layers copies of the final block channel_out, because last downblock has no downsampling
        
        length of list will be, therefore, 1 + (num_blocks-1)*(num_layers+1) + num_layers = num_blocks*(num_layers+1),
        with (num_layers+1) being the number of skip connections going into each upblock
        '''
        skip_conn_channels=[block_channel_counts[0]]
        for i in range(num_blocks - 1):
            skip_conn_channels = skip_conn_channels + (num_layers+1) * [block_channel_counts[i]]
        skip_conn_channels = skip_conn_channels + num_layers * [block_channel_counts[-1]]
        
        skips_per_block = num_layers+1
        skip_conn_channels = list(reversed(skip_conn_channels))
        skip_conn_channels = [skip_conn_channels[i: i+skips_per_block] for i in range(0, len(skip_conn_channels), skips_per_block)]
        self.skip_conn_channels = skip_conn_channels
            
        prev_channels = block_channel_counts[-1]
        final = False
        self.upblocks=[]
        for i in range(num_blocks):
            if i==num_blocks-1:
                final = True
            
            erb_channels_per_head = channels_per_head if i < (num_blocks - attn_start) else 0
            up_out_channels=block_channel_counts[-(i+1)]
            block = UpBlock(final_emb_sz, prev_channels, skip_conn_channels[i], up_out_channels, final, num_layers+1, erb_channels_per_head)
            self.upblocks.append(block)
            prev_channels = up_out_channels
        
        self.upblocks = nn.ModuleList(self.upblocks)
        
        # last conv
        self.last_conv= pre_conv(block_channel_counts[0], out_channels, stride=1, act=nn.SiLU, norm=nn.BatchNorm2d, bias=False)
        self.last_conv.in_channels = block_channel_counts[0]
        self.last_conv.out_channels = out_channels
        
    '''
    Model takes t tensor with a different timestep for each image, shape [batch_size]
    '''
    def forward(self, inp):
        x, t, c = inp
        t_emb = timestep_embedding(t, self.t_emb_sz)
        c_emb = self.c_emb(c)
        '''
        t is "double embedded" first from a
        '''
        generic_emb = self.t_emb_mlp(t_emb) + c_emb
        x = self.first_conv(x)
        skip_conns = [x]
        for block in self.downblocks:
            x = block(x, generic_emb)
            skip_conns = skip_conns + block.saved_out
        
        x = self.midblock(x, generic_emb)
        
        for block  in self.upblocks:
            x = block(x, generic_emb, skip_conns)
        
        return self.last_conv(x)
        

# %% ../nbs/28 copy attention only.ipynb 73
def cond_sample(clas, denoise_fn, denoise_mdl, sz, steps, eta=1., clamp=True):
    clas = torch.tensor(clas).cuda()[None]
    denoise_mdl.eval()
    xt = torch.randn(sz).cuda()
    step_sz = 1/steps
    ts = torch.linspace(1-step_sz, 0, steps).cuda()
    for i,t in enumerate(progress_bar(ts)):
        with torch.no_grad():
            pred_noise = denoise_mdl((xt, t[None], clas))
            abar_t=abar(t)
            abar_tm1=abar(t - step_sz) if t >= step_sz else torch.tensor(1).cuda()
            x0hat, xt = denoise_fn(xt, pred_noise, abar_t, abar_tm1, 1-abar_t, 1-abar_tm1, eta, 1-((i+1)/100), clamp=clamp)
    return x0hat.float().cpu()
        
