# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/14 copy data augmentation.ipynb.

# %% auto 0
__all__ = ['summary', 'show_image_batch', 'CapturePreds', 'capture_preds', 'rand_erase', 'RandErase', 'rand_copy', 'RandCopy']

# %% ../nbs/14 copy data augmentation.ipynb 2
import torch,random
import fastcore.all as fc

from torch import nn
from torch.nn import init

from .datasets import *
from .conv import *
from .learner import *
from .activations import *
from .init import *
from .sgd import *
from .resnet import *

# %% ../nbs/14 copy data augmentation.ipynb 18
def _flops(x, h, w):
    if x.dim()<3: return x.numel()
    if x.dim()==4: return x.numel()*h*w

@fc.patch
def summary(self:Learner):
    res = '|Module|Input|Output|Num params|MFLOPS|\n|--|--|--|--|--|\n'
    totp,totf = 0,0
    def _f(hook, mod, inp, outp):
        nonlocal res,totp,totf
        nparms = sum(o.numel() for o in mod.parameters())
        totp += nparms
        *_,h,w = outp.shape
        flops = sum(_flops(o, h, w) for o in mod.parameters())/1e6
        totf += flops
        res += f'|{type(mod).__name__}|{tuple(inp[0].shape)}|{tuple(outp.shape)}|{nparms}|{flops:.1f}|\n'
    with Hooks(self.model, _f) as hooks: self.fit(1, lr=1, do_train=False, do_validate=True, cbs=SingleBatchCB())
    print(f"Tot params: {totp}; MFLOPS: {totf:.1f}")
    if fc.IN_NOTEBOOK:
        from IPython.display import Markdown
        return Markdown(res)
    else: print(res)

# %% ../nbs/14 copy data augmentation.ipynb 36
@fc.patch
@fc.delegates(show_images)
def show_image_batch(self:Learner, max_n=9, cbs=None, **kwargs):
    self.fit(1, cbs=[SingleBatchCB()]+fc.L(cbs))
    show_images(self.batch[0][:max_n], **kwargs)

# %% ../nbs/14 copy data augmentation.ipynb 59
class CapturePreds(Callback):
    def before_fit(self, learn): self.all_inps,self.all_preds,self.all_targs = torch.tensor([]),torch.tensor([]),torch.tensor([])
    def after_batch(self, learn):
        # self.all_inps. append(to_cpu(learn.batch[0]))
        self.all_inps = torch.cat((self.all_inps, to_cpu(learn.batch[0])))
        # self.all_preds.append(to_cpu(learn.preds))
        self.all_preds = torch.cat((self.all_preds, to_cpu(learn.preds)))
        # self.all_targs.append(to_cpu(learn.batch[1]))
        self.all_targs = torch.cat((self.all_targs, to_cpu(learn.batch[1])))

    # def after_fit(self, learn):
    #     self.all_preds,self.all_targs,self.all_inps = map(torch.cat, [self.all_preds,self.all_targs,self.all_inps])

    
    
# class CapturePreds(Callback):
#     def before_fit(self, learn):
#         self.all_inps = []
#         self.all_preds = []
#         self.all_targs = []
    
#     def after_batch(self, learn):
#         self.all_inps.append(to_cpu(learn.batch[0]))
#         self.all_preds.append(to_cpu(learn.preds))
#         self.all_targs.append(to_cpu(learn.batch[1]))
                              
#     def after_fit(self, learn):
#         self.all_inps  = torch.cat( self.all_inps )
#         self.all_preds = torch.cat( self.all_preds)                    
#         self.all_targs = torch.cat( self.all_targs)

# %% ../nbs/14 copy data augmentation.ipynb 60
@fc.patch
def capture_preds(self: Learner, cbs=None, inps=False):
    predscb = CapturePreds()
    self.fit(1, do_train=False, cbs=[predscb]+fc.L(cbs))
    res = predscb.all_preds, predscb.all_targs
    if inps:
        res = res + (predscb.all_inps,)
    
    return res

# %% ../nbs/14 copy data augmentation.ipynb 64
def _rand_erase1(x, pct, xmean, xstd, xmin, xmax):
    size_x = int(pct * x.shape[-2])
    size_y = int(pct * x.shape[-1])
    start_x = int((1-pct) * x.shape[-2] * random.random())
    start_y = int((1-pct) * x.shape[-1] * random.random())
    init.normal_(x[:, :, start_x:start_x+size_x, start_y:start_y+size_y], xmean, xstd)
    x.clamp_(xmin, xmax)

# %% ../nbs/14 copy data augmentation.ipynb 65
def rand_erase(x, pct=0.2, max_num=4):
    for i in range(random.randint(0, max_num)):
        _rand_erase1(x, pct, x.mean(), x.std(), x.min(), x.max())
    return x
                   
class RandErase(nn.Module):
    def __init__(self, pct=0.2, max_num=4):
        super().__init__()
        self.pct, self.max_num = pct, max_num
    
    def forward(self, x):
        return rand_erase(x, self.pct, self.max_num)

# %% ../nbs/14 copy data augmentation.ipynb 78
def _rand_copy1(x, pct):
    szx = int(pct*x.shape[-2])
    szy = int(pct*x.shape[-1])
    stx1 = int(random.random()*(1-pct)*x.shape[-2])
    sty1 = int(random.random()*(1-pct)*x.shape[-1])
    stx2 = int(random.random()*(1-pct)*x.shape[-2])
    sty2 = int(random.random()*(1-pct)*x.shape[-1])
    x[:,:,stx1:stx1+szx,sty1:sty1+szy] = x[:,:,stx2:stx2+szx,sty2:sty2+szy]

# %% ../nbs/14 copy data augmentation.ipynb 79
def rand_copy(x, pct=0.2, max_num = 4):
    num = random.randint(0, max_num)
    for i in range(num): _rand_copy1(x, pct)
#     print(num)
    return x

# %% ../nbs/14 copy data augmentation.ipynb 80
class RandCopy(nn.Module):
    def __init__(self, pct=0.2, max_num=4):
        super().__init__()
        self.pct,self.max_num = pct,max_num
    def forward(self, x): return rand_copy(x, self.pct, self.max_num)
