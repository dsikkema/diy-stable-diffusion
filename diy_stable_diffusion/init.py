# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/11 copy initialization.ipynb.

# %% auto 0
__all__ = ['clean_ipython_hist', 'clean_tb', 'clean_mem', 'BatchTransformCB', 'GeneralRelu', 'plot_func', 'init_weights',
           'lsuv_init', 'conv', 'get_model']

# %% ../nbs/11 copy initialization.ipynb 1
import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt
import sys,gc,traceback
import fastcore.all as fc
from collections.abc import Mapping
from pathlib import Path
from operator import attrgetter,itemgetter
from functools import partial
from copy import copy
from contextlib import contextmanager

import torchvision.transforms.functional as TF,torch.nn.functional as F
from torch import tensor,nn,optim
from torch.utils.data import DataLoader,default_collate
from torch.nn import init
from torcheval.metrics import MulticlassAccuracy
from datasets import load_dataset,load_dataset_builder

from .datasets import *
from .conv import *
from .learner import *
from .activations import *

# %% ../nbs/11 copy initialization.ipynb 13
def clean_ipython_hist():
    # Code in this function mainly copied from IPython source
    if not 'get_ipython' in globals(): return
    ip = get_ipython()
    user_ns = ip.user_ns
    ip.displayhook.flush()
    pc = ip.displayhook.prompt_count + 1
    for n in range(1, pc): user_ns.pop('_i'+repr(n),None)
    user_ns.update(dict(_i='',_ii='',_iii=''))
    hm = ip.history_manager
    hm.input_hist_parsed[:] = [''] * pc
    hm.input_hist_raw[:] = [''] * pc
    hm._i = hm._ii = hm._iii = hm._i00 =  ''

# %% ../nbs/11 copy initialization.ipynb 14
def clean_tb():
    # h/t Piotr Czapla
    if hasattr(sys, 'last_traceback'):
        traceback.clear_frames(sys.last_traceback)
        delattr(sys, 'last_traceback')
    if hasattr(sys, 'last_type'): delattr(sys, 'last_type')
    if hasattr(sys, 'last_value'): delattr(sys, 'last_value')
    

# %% ../nbs/11 copy initialization.ipynb 15
def clean_mem():
    clean_tb()
    clean_ipython_hist()
    gc.collect()
    torch.cuda.empty_cache()

# %% ../nbs/11 copy initialization.ipynb 38
class BatchTransformCB(Callback):
    def __init__(self, transform, on_train=True, on_val=True):
        fc.store_attr()
    
    def before_batch(self, learn):
        if (self.on_train and learn.training) or (self.on_val and not learn.training):
            learn.batch = self.transform(learn.batch)

# %% ../nbs/11 copy initialization.ipynb 45
class GeneralRelu(nn.Module):
    def __init__(self, leak=None, sub=None, maxv=None):
        super().__init__()
        fc.store_attr()
    
    def forward(self, x):
        x = F.leaky_relu(x, self.leak) if self.leak is not None else F.relu(x)
        if self.sub is not None:
            x -= self.sub
        
        if self.maxv is not None:
            x.clamp_max_(self.maxv)
        return x


# %% ../nbs/11 copy initialization.ipynb 47
def plot_func(f, start=-5., end=5., steps=100):
    x=torch.linspace(start, end, steps)
    plt.plot(x, f(x))
    plt.grid(True, which='both', ls='--')
    plt.axhline(y=0, color='k', linewidth=0.7)
    plt.axvline(x=0, color='k', linewidth=0.7)

# %% ../nbs/11 copy initialization.ipynb 51
def init_weights(model, leaky=0.0):
    if isinstance(model, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):
        init.kaiming_normal_(model.weight, a=leaky)

# %% ../nbs/11 copy initialization.ipynb 55
def _lsuv_stats(hook, mod, inp, outp):
    acts = to_cpu(outp)
    hook.mean = acts.mean()
    hook.std = acts.std()

def lsuv_init(model, act_module, weight_module, xb):
    h = Hook(act_module, _lsuv_stats)
    with torch.no_grad():
        model(xb)
        while (abs(h.std-1) > 1e-3 or abs(h.mean) > 1e-3):
            weight_module.bias -= h.mean
            weight_module.weight.data /= h.std
            model(xb)
    h.remove()

# %% ../nbs/11 copy initialization.ipynb 65
def conv(in_chans, out_chans, kernel_size=3,stride=2,activation=nn.ReLU,norm=None,bias=None):
    if bias is None:
        bias = not isinstance(norm, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d))
    
    layers = [nn.Conv2d(in_chans, out_chans, stride=stride, kernel_size=kernel_size, padding=kernel_size//2, bias=bias)]
    
    if norm:
        layers.append(norm(out_chans))
    
    if activation:
        layers.append(activation())
    
    return nn.Sequential(*layers)

# %% ../nbs/11 copy initialization.ipynb 66
def get_model(activation=nn.ReLU, channel_layers=None, norm=None):
    if channel_layers is None:
        channel_layers = [1,8,16,32,64]
    
    layers = [conv(channel_layers[i],
                   channel_layers[i+1],
                   activation=activation,
                   norm=norm
                  ) for i in range(len(channel_layers) - 1)] + [
        conv(channel_layers[-1], 10, activation=None, norm=False, bias=True),
        nn.Flatten()
    ]
    
    return nn.Sequential(*layers).to(def_device)

